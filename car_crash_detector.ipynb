{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "56lwEQVts72M",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5028cbc5-6f40-4781-9c9f-265b7013d186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "r5K5XLrUGMki",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "840cb58a-356d-410e-d271-97007ad9187d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
      "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
      "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
      "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  match = re.search('\\d+$', rotation_line)\n",
      "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if event.key is 'enter':\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from moviepy.editor import VideoFileClip\n",
    "from keras import layers, applications\n",
    "from datasets import load_dataset\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.nn.functional as FF\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2, os, gc, torch, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "abe0152dcb4746ffa5b0a0a9c4362112",
      "9fb1862e23eb46248ee01b97991466a2",
      "373b575f32234aa5913a9d64b6522e63",
      "fb8bb708dbc2470aa508e40322be7029",
      "0cd409820c954b68866aa9e574439969",
      "c5b2730ae68741c989d055165056a4a0",
      "53a9306d22ba4831967a5605b7bc83b7",
      "46e4786285e644bca3782cf0525486e4",
      "b44b34da7ace422f9979727581848f0e",
      "a5c7c2d2af694d7bb5001e881ec8db4e",
      "3fbc91a36afb48ad9f16615725ec72fa",
      "95f5a502602141578e044800413bd614",
      "e0285977ee57455894d11ec09ba65263",
      "6e7eee7a1e164d9eadd601348c63b5b7",
      "f2a953468663430c8762dad57a6dbe0b",
      "346426a78f35469d8c73c96d05337edf",
      "856832b09a1547139667b0fe699a83a2",
      "1b472e8deb404e55b268321747e64bbf",
      "7ef02a466dbf478a81ac0a9d74d5a656",
      "f012e13d1592483db2d9524f172ca444",
      "0fb16adf17394f9f9500807b53e433c9",
      "03b4df6d3c3b4936a3cd87d142f59a27"
     ]
    },
    "id": "299ub9AbtTYA",
    "outputId": "3b4e01f3-791e-4dd6-ad50-6c7a8f5632b2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe0152dcb4746ffa5b0a0a9c4362112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f5a502602141578e044800413bd614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while True :\n",
    "    try :\n",
    "        ds = load_dataset('nexar-ai/nexar_collision_prediction', split='train')\n",
    "        break\n",
    "\n",
    "    except :\n",
    "        time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1TwOr4Ad_tsC"
   },
   "outputs": [],
   "source": [
    "n = round(ds.num_rows * 0.7)\n",
    "subset = ds.shuffle(seed=1404).select(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UdBa8Xtvtcs7"
   },
   "outputs": [],
   "source": [
    "train_df = subset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7f11eX2VbE_T"
   },
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['time_of_event'].apply(lambda x: 0 if pd.isna(x) or x is None else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mbF_a676cyPI"
   },
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(\n",
    "    train_df,\n",
    "    columns=['light_conditions', 'weather', 'scene'],\n",
    "    prefix=['light', 'weather', 'scene'],\n",
    "    dtype=int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7781WD8Whpv9"
   },
   "outputs": [],
   "source": [
    "train_df_split, val_df_split = train_test_split(train_df, test_size=0.3, random_state=1404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81feoejPGdHd",
    "outputId": "654bff6b-812c-488c-8333-13bc26457878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 735/735 [18:01<00:00,  1.47s/it]\n",
      "100%|██████████| 315/315 [07:29<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'video_matrices'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def resize_on_gpu(frames):\n",
    "    # Convert (N, H, W, C) -> (N, C, H, W)\n",
    "    frames_tensor = torch.from_numpy(frames).permute(0, 3, 1, 2).float().to(device)\n",
    "    # Resize to (224, 224)\n",
    "    resized = F.resize(frames_tensor, [224, 224])\n",
    "    # Back to (N, H, W, C)\n",
    "    return resized.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "def video_to_matrix(video_df, is_train=True, frame_interval=6):\n",
    "    for idx, row in tqdm(video_df.iterrows(), total=len(video_df)):\n",
    "        video_path = row['video']['path']\n",
    "        time_of_event = row['time_of_event']\n",
    "        label = row['label']\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f'[WARN] Cannot open video at index {idx}: {video_path}')\n",
    "            continue\n",
    "\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps == 0 or fps is None:\n",
    "            print(f'[WARN] FPS is zero at index {idx}')\n",
    "            cap.release()\n",
    "            continue\n",
    "\n",
    "        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        duration = total_frames / fps\n",
    "\n",
    "        if label :\n",
    "            start_time = time_of_event - 2\n",
    "            end_time = time_of_event\n",
    "        else :\n",
    "            random_time = np.random.uniform(0, duration - 2)\n",
    "            start_time = random_time\n",
    "            end_time = random_time + 2\n",
    "\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = int(end_time * fps)\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        frames = []\n",
    "        frame_index = start_frame\n",
    "\n",
    "        while frame_index <= end_frame:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            if frame_index % frame_interval == 0:\n",
    "                frames.append(frame)\n",
    "            frame_index += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if frames:\n",
    "            frames = np.stack(frames)  # (N, H, W, 3)\n",
    "            video_matrix = resize_on_gpu(frames)  # GPU accelerated resize\n",
    "            save_path = os.path.join(output_dir, f'video_{idx}.npy')\n",
    "            np.save(save_path, video_matrix)\n",
    "\n",
    "        del frames\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "# Run it\n",
    "video_to_matrix(train_df_split)\n",
    "video_to_matrix(val_df_split, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "I_tHhlu4acKv"
   },
   "outputs": [],
   "source": [
    "def add_matrix_path_col(df) :\n",
    "  output_dir = 'video_matrices'\n",
    "  df['video_matrix_path'] = None\n",
    "\n",
    "  for idx in df.index:\n",
    "      file_path = os.path.join(output_dir, f'video_{idx}.npy')\n",
    "      if os.path.exists(file_path):\n",
    "          df.at[idx, 'video_matrix_path'] = file_path\n",
    "      else:\n",
    "          df.at[idx, 'video_matrix_path'] = None\n",
    "\n",
    "add_matrix_path_col(train_df_split)\n",
    "add_matrix_path_col(val_df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHBcFHrN_LWS",
    "outputId": "0f77e0e0-5c1e-41ac-d8b9-686251cc856d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 735/735 [07:49<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [03:19<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "def masked_video_to_matrix(df):\n",
    "    output_dir = 'masked_video_matrices'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    for idx, video in tqdm(df.iterrows(), total=len(df)):\n",
    "        video_path = '/content/' + video['video_matrix_path']\n",
    "        video_frames = np.load(video_path)  # (N, H, W, 3)\n",
    "        frames_tensor = torch.from_numpy(video_frames).permute(0, 3, 1, 2).float().to(device) / 255.0\n",
    "        num_frames = frames_tensor.shape[0]\n",
    "\n",
    "        mask_frames = []\n",
    "\n",
    "        # Convert to grayscale on GPU\n",
    "        gray = 0.2989 * frames_tensor[:, 0] + 0.5870 * frames_tensor[:, 1] + 0.1140 * frames_tensor[:, 2]\n",
    "\n",
    "        for i in range(1, num_frames):\n",
    "            prev_gray = gray[i - 1:i]  # (1, H, W)\n",
    "            next_gray = gray[i:i + 1]\n",
    "\n",
    "            # Optical flow approximation using spatial gradients\n",
    "            # Compute gradients with Sobel kernels\n",
    "            sobel_x = torch.tensor([[1, 0, -1],\n",
    "                                    [2, 0, -2],\n",
    "                                    [1, 0, -1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
    "            sobel_y = torch.tensor([[1, 2, 1],\n",
    "                                    [0, 0, 0],\n",
    "                                    [-1, -2, -1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
    "\n",
    "            Ix = FF.conv2d(prev_gray.unsqueeze(0), sobel_x, padding=1)\n",
    "            Iy = FF.conv2d(prev_gray.unsqueeze(0), sobel_y, padding=1)\n",
    "            It = next_gray.unsqueeze(0) - prev_gray.unsqueeze(0)\n",
    "\n",
    "            # Lucas–Kanade flow per-pixel in 3×3 window\n",
    "            kernel = torch.ones((1, 1, 3, 3), device=device)\n",
    "            Ixx = FF.conv2d(Ix * Ix, kernel, padding=1)\n",
    "            Iyy = FF.conv2d(Iy * Iy, kernel, padding=1)\n",
    "            Ixy = FF.conv2d(Ix * Iy, kernel, padding=1)\n",
    "            Ixt = FF.conv2d(Ix * It, kernel, padding=1)\n",
    "            Iyt = FF.conv2d(Iy * It, kernel, padding=1)\n",
    "\n",
    "            det = Ixx * Iyy - Ixy * Ixy + 1e-6\n",
    "            u = (Iyy * (-Ixt) - Ixy * (-Iyt)) / det\n",
    "            v = (-Ixy * (-Ixt) + Ixx * (-Iyt)) / det\n",
    "\n",
    "            u, v = u[0, 0], v[0, 0]  # (H, W)\n",
    "\n",
    "            # Magnitude / angle\n",
    "            magnitude = torch.sqrt(u ** 2 + v ** 2)\n",
    "            angle = torch.atan2(v, u) * (180.0 / np.pi / 2.0)\n",
    "\n",
    "            # Normalize magnitude → [0,255]\n",
    "            magnitude = 255 * (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-6)\n",
    "\n",
    "            # Divergence\n",
    "            dy_u, dx_u = torch.gradient(u)\n",
    "            dy_v, dx_v = torch.gradient(v)\n",
    "            divergence = dx_u + dy_v\n",
    "            divergence = torch.tanh(divergence)\n",
    "            divergence = ((divergence + 1.0) / 2.0) * 255\n",
    "\n",
    "            flow_channels = torch.stack([magnitude, angle, divergence], dim=0).clamp(0, 255)\n",
    "            mask_frames.append(flow_channels.to(\"cpu\", torch.uint8))\n",
    "\n",
    "        mask_frames = torch.stack(mask_frames).numpy()\n",
    "        save_path = os.path.join(output_dir, f'masked_video_{idx}.npy')\n",
    "        np.save(save_path, mask_frames)\n",
    "\n",
    "        del frames_tensor, mask_frames\n",
    "        gc.collect()\n",
    "\n",
    "masked_video_to_matrix(train_df_split)\n",
    "masked_video_to_matrix(val_df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "T-pH5wCoCbne"
   },
   "outputs": [],
   "source": [
    "def add_matrix_path_col(df) :\n",
    "  output_dir = 'masked_video_matrices'\n",
    "  df['masked_video_matrix_path'] = None\n",
    "\n",
    "  for idx in df.index:\n",
    "      file_path = os.path.join(output_dir, f'masked_video_{idx}.npy')\n",
    "      if os.path.exists(file_path) :\n",
    "          df.at[idx, 'masked_video_matrix_path'] = file_path\n",
    "      else:\n",
    "          df.at[idx, 'masked_video_matrix_path'] = None\n",
    "\n",
    "add_matrix_path_col(train_df_split)\n",
    "add_matrix_path_col(val_df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ajcc09yjuuyv"
   },
   "outputs": [],
   "source": [
    "train_df_split = train_df_split.reset_index()\n",
    "val_df_split = val_df_split.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fLCwPSBnwBEZ"
   },
   "outputs": [],
   "source": [
    "train_df_split = train_df_split.drop(columns=['video', 'time_of_event', 'time_of_alert', 'time_to_accident', 'index'])\n",
    "val_df_split = val_df_split.drop(columns=['video', 'time_of_event', 'time_of_alert', 'time_to_accident', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3Nh-5sN3SyGQ"
   },
   "outputs": [],
   "source": [
    "MAX_FRAMES = 10\n",
    "def pad_or_truncate(seq, max_len):\n",
    "    seq = np.array(seq)\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    if len(seq) < max_len:\n",
    "        last = seq[-1]\n",
    "        padding = np.repeat(last[None, ...], max_len - len(seq), axis=0)\n",
    "        return np.concatenate([seq, padding], axis=0)\n",
    "    return seq\n",
    "\n",
    "class VideoMatrixSequence(keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size=16, target_size=(224, 224), shuffle=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.shuffle = shuffle\n",
    "        self.columns = self.df.columns.drop(['video_matrix_path', 'masked_video_matrix_path', 'label'])\n",
    "        self.indices = np.arange(len(self.df))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "\n",
    "        tabular_data = []\n",
    "        video_data = []\n",
    "        mask_data = []\n",
    "        labels = []\n",
    "\n",
    "        for i in batch_indices :\n",
    "            row = self.df.iloc[i]\n",
    "\n",
    "            video = np.load(row['video_matrix_path'])\n",
    "            frames_resized = np.stack(video)\n",
    "            frames_resized = pad_or_truncate(frames_resized, MAX_FRAMES)\n",
    "\n",
    "            mask = np.load(row['masked_video_matrix_path'])\n",
    "            mask_frames = np.stack(mask)\n",
    "            mask_frames = pad_or_truncate(mask_frames, MAX_FRAMES)\n",
    "\n",
    "            tabular_features = [row[col] for col in self.columns]\n",
    "            tabular_data.append(tabular_features)\n",
    "\n",
    "            video_data.append(frames_resized)\n",
    "            mask_data.append(mask_frames)\n",
    "            labels.append(float(row['label']))\n",
    "\n",
    "        tabular_array = np.array(tabular_data, dtype=np.float32)\n",
    "        video_array = np.array(video_data, dtype=np.float32)\n",
    "        mask_array = np.array(mask_data, dtype=np.float32)\n",
    "        labels_array = np.array(labels, dtype=np.float32)\n",
    "        labels_array = np.expand_dims(labels_array, axis=-1)\n",
    "\n",
    "        output = (\n",
    "            {\n",
    "                'video': video_array,\n",
    "                'mask_flow': mask_array,\n",
    "                'tabular': tabular_array\n",
    "            }, labels_array\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d2wX7HylfN62"
   },
   "outputs": [],
   "source": [
    "train_seq = VideoMatrixSequence(train_df_split, batch_size=16, target_size=(224, 224))\n",
    "val_seq = VideoMatrixSequence(val_df_split, batch_size=16, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "j6ACOPvztaTY"
   },
   "outputs": [],
   "source": [
    "def augmentation_pipeline(noise_std=0.1, brightness_factor=0.2, contrast_factor=0.2):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.RandomBrightness(factor=brightness_factor),\n",
    "        layers.RandomContrast(factor=contrast_factor),\n",
    "        layers.GaussianNoise(noise_std)\n",
    "    ], name=\"augmentation_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uqop2UXY-BFW"
   },
   "outputs": [],
   "source": [
    "def mask_cnn_net(input_shape=(3, 224, 224), dropout_rate=0.5):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Permute((2, 3, 1))(inputs)\n",
    "    def preprocess_resnet(x):\n",
    "        return applications.resnet50.preprocess_input(x)\n",
    "\n",
    "    x = layers.TimeDistributed(layers.Lambda(preprocess_resnet))(x)\n",
    "    # x = layers.TimeDistributed(augmentation_pipeline())(x)\n",
    "\n",
    "    base = applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=x,\n",
    "        pooling=None\n",
    "    )\n",
    "    base.trainable = False\n",
    "\n",
    "    x = base.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    return keras.Model(inputs, x, name='mask_flow_backbone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBRxKmhYV2yz",
    "outputId": "29430d34-1d3d-4266-f434-f39de72a93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5270 - auc: 0.5265 - f1: 0.5114 - loss: 0.9808 - precision: 0.5015 - recall: 0.5228\n",
      "Epoch 1: val_accuracy improved from -inf to 0.74671, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 5s/step - accuracy: 0.5280 - auc: 0.5282 - f1: 0.5126 - loss: 0.9788 - precision: 0.5028 - recall: 0.5238 - val_accuracy: 0.7467 - val_auc: 0.8470 - val_f1: 0.6778 - val_loss: 0.5263 - val_precision: 0.8710 - val_recall: 0.5548\n",
      "Epoch 2/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6819 - auc: 0.7607 - f1: 0.6708 - loss: 0.6334 - precision: 0.6701 - recall: 0.6727\n",
      "Epoch 2: val_accuracy improved from 0.74671 to 0.77303, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - accuracy: 0.6822 - auc: 0.7609 - f1: 0.6713 - loss: 0.6331 - precision: 0.6706 - recall: 0.6730 - val_accuracy: 0.7730 - val_auc: 0.8626 - val_f1: 0.7629 - val_loss: 0.4744 - val_precision: 0.7708 - val_recall: 0.7551\n",
      "Epoch 3/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8002 - auc: 0.8609 - f1: 0.7991 - loss: 0.4802 - precision: 0.7873 - recall: 0.8116\n",
      "Epoch 3: val_accuracy improved from 0.77303 to 0.80592, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.8000 - auc: 0.8609 - f1: 0.7990 - loss: 0.4802 - precision: 0.7871 - recall: 0.8116 - val_accuracy: 0.8059 - val_auc: 0.8887 - val_f1: 0.8078 - val_loss: 0.4403 - val_precision: 0.7607 - val_recall: 0.8611\n",
      "Epoch 4/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7899 - auc: 0.8919 - f1: 0.7920 - loss: 0.4149 - precision: 0.7832 - recall: 0.8018\n",
      "Epoch 4: val_accuracy improved from 0.80592 to 0.80921, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 3s/step - accuracy: 0.7898 - auc: 0.8916 - f1: 0.7918 - loss: 0.4156 - precision: 0.7830 - recall: 0.8017 - val_accuracy: 0.8092 - val_auc: 0.8884 - val_f1: 0.8014 - val_loss: 0.4245 - val_precision: 0.8125 - val_recall: 0.7905\n",
      "Epoch 5/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8197 - auc: 0.8899 - f1: 0.8217 - loss: 0.4206 - precision: 0.8258 - recall: 0.8182\n",
      "Epoch 5: val_accuracy improved from 0.80921 to 0.82237, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 3s/step - accuracy: 0.8197 - auc: 0.8900 - f1: 0.8217 - loss: 0.4205 - precision: 0.8257 - recall: 0.8181 - val_accuracy: 0.8224 - val_auc: 0.8903 - val_f1: 0.8112 - val_loss: 0.4206 - val_precision: 0.8406 - val_recall: 0.7838\n",
      "Epoch 6/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8233 - auc: 0.9191 - f1: 0.8109 - loss: 0.3648 - precision: 0.7999 - recall: 0.8242\n",
      "Epoch 6: val_accuracy improved from 0.82237 to 0.84211, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 3s/step - accuracy: 0.8238 - auc: 0.9192 - f1: 0.8117 - loss: 0.3645 - precision: 0.8009 - recall: 0.8246 - val_accuracy: 0.8421 - val_auc: 0.8953 - val_f1: 0.8322 - val_loss: 0.4088 - val_precision: 0.8500 - val_recall: 0.8151\n",
      "Epoch 7/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8302 - auc: 0.9326 - f1: 0.8296 - loss: 0.3359 - precision: 0.8548 - recall: 0.8082\n",
      "Epoch 7: val_accuracy did not improve from 0.84211\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.8306 - auc: 0.9326 - f1: 0.8299 - loss: 0.3357 - precision: 0.8546 - recall: 0.8091 - val_accuracy: 0.8322 - val_auc: 0.8992 - val_f1: 0.8235 - val_loss: 0.4015 - val_precision: 0.8380 - val_recall: 0.8095\n",
      "Epoch 8/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8773 - auc: 0.9436 - f1: 0.8812 - loss: 0.3083 - precision: 0.8789 - recall: 0.8838\n",
      "Epoch 8: val_accuracy did not improve from 0.84211\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.8776 - auc: 0.9438 - f1: 0.8814 - loss: 0.3078 - precision: 0.8788 - recall: 0.8842 - val_accuracy: 0.8322 - val_auc: 0.8964 - val_f1: 0.8132 - val_loss: 0.4206 - val_precision: 0.8605 - val_recall: 0.7708\n",
      "Epoch 9/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8792 - auc: 0.9521 - f1: 0.8766 - loss: 0.2871 - precision: 0.8726 - recall: 0.8826\n",
      "Epoch 9: val_accuracy did not improve from 0.84211\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 2s/step - accuracy: 0.8792 - auc: 0.9522 - f1: 0.8766 - loss: 0.2869 - precision: 0.8730 - recall: 0.8822 - val_accuracy: 0.8191 - val_auc: 0.8803 - val_f1: 0.8029 - val_loss: 0.4579 - val_precision: 0.8550 - val_recall: 0.7568\n",
      "Epoch 10/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9066 - auc: 0.9706 - f1: 0.9059 - loss: 0.2270 - precision: 0.8873 - recall: 0.9263\n",
      "Epoch 10: val_accuracy improved from 0.84211 to 0.84868, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 3s/step - accuracy: 0.9062 - auc: 0.9705 - f1: 0.9055 - loss: 0.2275 - precision: 0.8872 - recall: 0.9257 - val_accuracy: 0.8487 - val_auc: 0.9126 - val_f1: 0.8392 - val_loss: 0.3823 - val_precision: 0.8511 - val_recall: 0.8276\n",
      "Epoch 11/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9112 - auc: 0.9771 - f1: 0.9129 - loss: 0.2051 - precision: 0.9086 - recall: 0.9177\n",
      "Epoch 11: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9112 - auc: 0.9769 - f1: 0.9127 - loss: 0.2055 - precision: 0.9086 - recall: 0.9175 - val_accuracy: 0.8289 - val_auc: 0.8987 - val_f1: 0.8255 - val_loss: 0.4173 - val_precision: 0.8092 - val_recall: 0.8425\n",
      "Epoch 12/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8914 - auc: 0.9761 - f1: 0.8939 - loss: 0.2207 - precision: 0.9179 - recall: 0.8734\n",
      "Epoch 12: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - accuracy: 0.8914 - auc: 0.9760 - f1: 0.8939 - loss: 0.2209 - precision: 0.9172 - recall: 0.8739 - val_accuracy: 0.8289 - val_auc: 0.8979 - val_f1: 0.8278 - val_loss: 0.4285 - val_precision: 0.7962 - val_recall: 0.8621\n",
      "Epoch 13/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9348 - auc: 0.9813 - f1: 0.9350 - loss: 0.1809 - precision: 0.9159 - recall: 0.9558\n",
      "Epoch 13: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9348 - auc: 0.9813 - f1: 0.9349 - loss: 0.1808 - precision: 0.9161 - recall: 0.9554 - val_accuracy: 0.8125 - val_auc: 0.9068 - val_f1: 0.8213 - val_loss: 0.4337 - val_precision: 0.7572 - val_recall: 0.8973\n",
      "Epoch 14/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9407 - auc: 0.9911 - f1: 0.9407 - loss: 0.1573 - precision: 0.9745 - recall: 0.9103\n",
      "Epoch 14: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9408 - auc: 0.9911 - f1: 0.9408 - loss: 0.1571 - precision: 0.9742 - recall: 0.9107 - val_accuracy: 0.8257 - val_auc: 0.9011 - val_f1: 0.8274 - val_loss: 0.4355 - val_precision: 0.7987 - val_recall: 0.8581\n",
      "Epoch 15/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9576 - auc: 0.9935 - f1: 0.9560 - loss: 0.1319 - precision: 0.9611 - recall: 0.9514\n",
      "Epoch 15: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9573 - auc: 0.9934 - f1: 0.9557 - loss: 0.1323 - precision: 0.9607 - recall: 0.9512 - val_accuracy: 0.8059 - val_auc: 0.9006 - val_f1: 0.7774 - val_loss: 0.4670 - val_precision: 0.8729 - val_recall: 0.7007\n",
      "Epoch 16/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9529 - auc: 0.9897 - f1: 0.9519 - loss: 0.1460 - precision: 0.9365 - recall: 0.9691\n",
      "Epoch 16: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9529 - auc: 0.9897 - f1: 0.9519 - loss: 0.1459 - precision: 0.9368 - recall: 0.9688 - val_accuracy: 0.8289 - val_auc: 0.9082 - val_f1: 0.8116 - val_loss: 0.4390 - val_precision: 0.8960 - val_recall: 0.7417\n",
      "Epoch 17/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9685 - auc: 0.9959 - f1: 0.9677 - loss: 0.1130 - precision: 0.9564 - recall: 0.9795\n",
      "Epoch 17: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.9683 - auc: 0.9958 - f1: 0.9676 - loss: 0.1132 - precision: 0.9562 - recall: 0.9792 - val_accuracy: 0.8125 - val_auc: 0.8916 - val_f1: 0.8190 - val_loss: 0.4819 - val_precision: 0.7771 - val_recall: 0.8658\n",
      "Epoch 18/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9766 - auc: 0.9968 - f1: 0.9756 - loss: 0.0893 - precision: 0.9730 - recall: 0.9783\n",
      "Epoch 18: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9765 - auc: 0.9968 - f1: 0.9754 - loss: 0.0895 - precision: 0.9729 - recall: 0.9781 - val_accuracy: 0.8059 - val_auc: 0.9016 - val_f1: 0.8196 - val_loss: 0.4970 - val_precision: 0.7444 - val_recall: 0.9116\n",
      "Epoch 19/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9558 - auc: 0.9923 - f1: 0.9565 - loss: 0.1263 - precision: 0.9638 - recall: 0.9495\n",
      "Epoch 19: val_accuracy did not improve from 0.84868\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - accuracy: 0.9559 - auc: 0.9923 - f1: 0.9565 - loss: 0.1261 - precision: 0.9639 - recall: 0.9496 - val_accuracy: 0.8092 - val_auc: 0.8875 - val_f1: 0.8067 - val_loss: 0.4966 - val_precision: 0.7707 - val_recall: 0.8462\n",
      "Epoch 20/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9777 - auc: 0.9960 - f1: 0.9773 - loss: 0.0901 - precision: 0.9757 - recall: 0.9789\n",
      "Epoch 20: val_accuracy improved from 0.84868 to 0.85526, saving model to best_video_model.keras\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 3s/step - accuracy: 0.9775 - auc: 0.9960 - f1: 0.9771 - loss: 0.0904 - precision: 0.9756 - recall: 0.9786 - val_accuracy: 0.8553 - val_auc: 0.9020 - val_f1: 0.8523 - val_loss: 0.4490 - val_precision: 0.8355 - val_recall: 0.8699\n",
      "Epoch 21/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9871 - auc: 0.9982 - f1: 0.9866 - loss: 0.0714 - precision: 0.9794 - recall: 0.9939\n",
      "Epoch 21: val_accuracy did not improve from 0.85526\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9872 - auc: 0.9983 - f1: 0.9866 - loss: 0.0712 - precision: 0.9796 - recall: 0.9939 - val_accuracy: 0.8224 - val_auc: 0.9028 - val_f1: 0.8258 - val_loss: 0.4618 - val_precision: 0.7853 - val_recall: 0.8707\n",
      "Epoch 22/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9579 - auc: 0.9914 - f1: 0.9588 - loss: 0.1225 - precision: 0.9599 - recall: 0.9580\n",
      "Epoch 22: val_accuracy did not improve from 0.85526\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9580 - auc: 0.9915 - f1: 0.9588 - loss: 0.1222 - precision: 0.9598 - recall: 0.9581 - val_accuracy: 0.8224 - val_auc: 0.8966 - val_f1: 0.8099 - val_loss: 0.4661 - val_precision: 0.8456 - val_recall: 0.7770\n",
      "Epoch 23/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9770 - auc: 0.9983 - f1: 0.9760 - loss: 0.0764 - precision: 0.9761 - recall: 0.9761\n",
      "Epoch 23: val_accuracy did not improve from 0.85526\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9768 - auc: 0.9983 - f1: 0.9758 - loss: 0.0765 - precision: 0.9759 - recall: 0.9759 - val_accuracy: 0.8125 - val_auc: 0.8914 - val_f1: 0.8167 - val_loss: 0.4970 - val_precision: 0.7791 - val_recall: 0.8581\n",
      "Epoch 24/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9859 - auc: 0.9993 - f1: 0.9857 - loss: 0.0615 - precision: 0.9814 - recall: 0.9900\n",
      "Epoch 24: val_accuracy did not improve from 0.85526\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 2s/step - accuracy: 0.9858 - auc: 0.9993 - f1: 0.9856 - loss: 0.0616 - precision: 0.9814 - recall: 0.9899 - val_accuracy: 0.8257 - val_auc: 0.8934 - val_f1: 0.8100 - val_loss: 0.5051 - val_precision: 0.8496 - val_recall: 0.7740\n",
      "Epoch 25/40\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9825 - auc: 0.9989 - f1: 0.9822 - loss: 0.0576 - precision: 0.9797 - recall: 0.9848\n",
      "Epoch 25: val_accuracy did not improve from 0.85526\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 2s/step - accuracy: 0.9825 - auc: 0.9989 - f1: 0.9822 - loss: 0.0575 - precision: 0.9798 - recall: 0.9848 - val_accuracy: 0.8289 - val_auc: 0.8984 - val_f1: 0.8312 - val_loss: 0.5041 - val_precision: 0.7950 - val_recall: 0.8707\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 10.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 224\n",
    "NUM_TABULAR_FEATURES = 14\n",
    "\n",
    "# Inputs\n",
    "video_input = keras.Input(shape=(MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3), name='video')\n",
    "mask_input = keras.Input(shape=(MAX_FRAMES, 3, IMG_SIZE, IMG_SIZE), name='mask_flow')\n",
    "tabular_input = keras.Input(shape=(NUM_TABULAR_FEATURES,), name='tabular')\n",
    "\n",
    "# Pre trained weights\n",
    "def preprocess_resnet(x):\n",
    "    return applications.resnet50.preprocess_input(x)\n",
    "\n",
    "video_x = layers.TimeDistributed(layers.Lambda(preprocess_resnet))(video_input)\n",
    "# video_x = layers.TimeDistributed(augmentation_pipeline())(video_x)\n",
    "\n",
    "# CNN Backbone\n",
    "base_cnn = applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "base_cnn.trainable = False\n",
    "\n",
    "video_features = layers.TimeDistributed(base_cnn)(video_x)\n",
    "mask_features = layers.TimeDistributed(mask_cnn_net())(mask_input)\n",
    "\n",
    "# GRU\n",
    "video_gru_out = layers.Bidirectional(layers.GRU(128, return_sequences=False))(video_features)\n",
    "video_gru_out = layers.Dropout(0.3)(video_gru_out)\n",
    "\n",
    "mask_gru_out = layers.Bidirectional(layers.GRU(128, return_sequences=False))(mask_features)\n",
    "mask_gru_out = layers.Dropout(0.3)(mask_gru_out)\n",
    "\n",
    "# Merge video and mask\n",
    "video_final = layers.Concatenate()([video_gru_out, mask_gru_out])\n",
    "\n",
    "# Tabular part\n",
    "tabular_dense = layers.Dense(64, activation='relu')(tabular_input)\n",
    "tabular_final = layers.Dropout(0.3)(tabular_dense)\n",
    "\n",
    "# Merge all\n",
    "final_merge = layers.Concatenate()([video_final, tabular_final])\n",
    "\n",
    "final_merge = layers.BatchNormalization()(final_merge)\n",
    "final_merge = layers.Dropout(0.3)(final_merge)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(final_merge)\n",
    "\n",
    "# Final model\n",
    "model = keras.Model(\n",
    "    inputs=[video_input, mask_input, tabular_input],\n",
    "    outputs=outputs,\n",
    "    name='final_model'\n",
    ")\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-5,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.FBetaScore(beta=1.0, threshold=0.5, average='macro', name='f1'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "cbs = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_video_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(train_seq, validation_data=val_seq, epochs=40, callbacks=cbs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03b4df6d3c3b4936a3cd87d142f59a27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0cd409820c954b68866aa9e574439969": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fb16adf17394f9f9500807b53e433c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b472e8deb404e55b268321747e64bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "346426a78f35469d8c73c96d05337edf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "373b575f32234aa5913a9d64b6522e63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46e4786285e644bca3782cf0525486e4",
      "max": 1502,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b44b34da7ace422f9979727581848f0e",
      "value": 1502
     }
    },
    "3fbc91a36afb48ad9f16615725ec72fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46e4786285e644bca3782cf0525486e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53a9306d22ba4831967a5605b7bc83b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e7eee7a1e164d9eadd601348c63b5b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ef02a466dbf478a81ac0a9d74d5a656",
      "max": 1348,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f012e13d1592483db2d9524f172ca444",
      "value": 1348
     }
    },
    "7ef02a466dbf478a81ac0a9d74d5a656": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "856832b09a1547139667b0fe699a83a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95f5a502602141578e044800413bd614": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e0285977ee57455894d11ec09ba65263",
       "IPY_MODEL_6e7eee7a1e164d9eadd601348c63b5b7",
       "IPY_MODEL_f2a953468663430c8762dad57a6dbe0b"
      ],
      "layout": "IPY_MODEL_346426a78f35469d8c73c96d05337edf"
     }
    },
    "9fb1862e23eb46248ee01b97991466a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5b2730ae68741c989d055165056a4a0",
      "placeholder": "​",
      "style": "IPY_MODEL_53a9306d22ba4831967a5605b7bc83b7",
      "value": "Resolving data files: 100%"
     }
    },
    "a5c7c2d2af694d7bb5001e881ec8db4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abe0152dcb4746ffa5b0a0a9c4362112": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fb1862e23eb46248ee01b97991466a2",
       "IPY_MODEL_373b575f32234aa5913a9d64b6522e63",
       "IPY_MODEL_fb8bb708dbc2470aa508e40322be7029"
      ],
      "layout": "IPY_MODEL_0cd409820c954b68866aa9e574439969"
     }
    },
    "b44b34da7ace422f9979727581848f0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c5b2730ae68741c989d055165056a4a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0285977ee57455894d11ec09ba65263": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_856832b09a1547139667b0fe699a83a2",
      "placeholder": "​",
      "style": "IPY_MODEL_1b472e8deb404e55b268321747e64bbf",
      "value": "Resolving data files: 100%"
     }
    },
    "f012e13d1592483db2d9524f172ca444": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2a953468663430c8762dad57a6dbe0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fb16adf17394f9f9500807b53e433c9",
      "placeholder": "​",
      "style": "IPY_MODEL_03b4df6d3c3b4936a3cd87d142f59a27",
      "value": " 1348/1348 [00:00&lt;00:00, 40396.12it/s]"
     }
    },
    "fb8bb708dbc2470aa508e40322be7029": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5c7c2d2af694d7bb5001e881ec8db4e",
      "placeholder": "​",
      "style": "IPY_MODEL_3fbc91a36afb48ad9f16615725ec72fa",
      "value": " 1502/1502 [00:00&lt;00:00, 68.69it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
